{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Regularization Effects on Parallel Circuits\n",
    "\n",
    "This notebook implements experiments to test how L1, L2, and dropout regularization affect parallel circuit emergence in neural networks.\n",
    "\n",
    "**Hypotheses:**\n",
    "- **L1 regularization** → Sparsity → Fewer parallel circuits\n",
    "- **L2 regularization** → Weight diffusion → More parallel circuits  \n",
    "- **Dropout** → Forced redundancy → More parallel circuits\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's verify GPU availability and clone the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    print(\"Warning: CUDA not available, using CPU\")\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "# Clone the MI-identifiability repository\n",
    "!git clone https://github.com/MelouxM/MI-identifiability.git\n",
    "%cd MI-identifiability\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "modifications-header"
   },
   "source": [
    "## Code Modifications\n",
    "\n",
    "Now we'll modify the code to add regularization support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "backup-original"
   },
   "outputs": [],
   "source": [
    "# Backup original files\n",
    "!cp mi_identifiability/neural_model.py mi_identifiability/neural_model.py.bak\n",
    "!cp main.py main.py.bak\n",
    "print(\"Original files backed up!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "modify-neural-model"
   },
   "outputs": [],
   "source": [
    "%%writefile mi_identifiability/neural_model.py\n",
    "import copy\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "ACTIVATION_FUNCTIONS = {\n",
    "    'leaky_relu': nn.LeakyReLU,\n",
    "    'relu': nn.ReLU,\n",
    "    'tanh': nn.Tanh,\n",
    "    'sigmoid': nn.Sigmoid\n",
    "}\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A class implementing a simple multi-layer perceptron model.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_sizes: list, input_size=2, output_size=1, \n",
    "                 activation='leaky_relu', device='cpu', dropout_rate=0.0):\n",
    "        \"\"\"\n",
    "        Initialize the MLP model.\n",
    "\n",
    "        Args:\n",
    "            hidden_sizes: A list of sizes of the hidden layers (in neurons), not including the input and output layers\n",
    "            input_size: The size of the input layer\n",
    "            output_size: The size of the output layer\n",
    "            activation: The activation function to use\n",
    "            device: The device to run the model on\n",
    "            dropout_rate: Dropout probability (0.0 = no dropout)\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Build layers with optional dropout\n",
    "        self.layers = nn.ModuleList()\n",
    "        for idx, (in_size, out_size) in enumerate(zip(self.layer_sizes, self.layer_sizes[1:])):\n",
    "            layer_components = [nn.Linear(in_size, out_size)]\n",
    "            \n",
    "            # Add dropout after linear layer (but not on output layer)\n",
    "            if idx < len(self.layer_sizes) - 2 and dropout_rate > 0:\n",
    "                layer_components.append(nn.Dropout(dropout_rate))\n",
    "            \n",
    "            # Add activation function (Identity for output layer)\n",
    "            if idx < len(self.layer_sizes) - 2:\n",
    "                layer_components.append(ACTIVATION_FUNCTIONS[activation]())\n",
    "            else:\n",
    "                layer_components.append(nn.Identity())\n",
    "            \n",
    "            self.layers.append(nn.Sequential(*layer_components))\n",
    "\n",
    "        self.num_layers = len(self.layers)\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def save_to_file(self, filepath):\n",
    "        \"\"\"\n",
    "        Save the model's configuration and state_dict to a file.\n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            'input_size': self.input_size,\n",
    "            'hidden_sizes': self.hidden_sizes,\n",
    "            'output_size': self.output_size,\n",
    "            'activation': self.activation,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'state_dict': self.state_dict()\n",
    "        }, filepath)\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_file(cls, filepath):\n",
    "        \"\"\"\n",
    "        Load a model from a file and return it.\n",
    "        \"\"\"\n",
    "        model_data = torch.load(filepath)\n",
    "        model = cls(\n",
    "            model_data['hidden_sizes'],\n",
    "            input_size=model_data['input_size'],\n",
    "            output_size=model_data['output_size'],\n",
    "            activation=model_data['activation'],\n",
    "            dropout_rate=model_data.get('dropout_rate', 0.0)\n",
    "        )\n",
    "        model.load_state_dict(model_data['state_dict'])\n",
    "        return model\n",
    "\n",
    "    def out_features(self, layer_id):\n",
    "        \"\"\"\n",
    "        Helper function to get the output size of a given layer\n",
    "\n",
    "        Args:\n",
    "            layer_id: The index of the layer\n",
    "\n",
    "        Returns:\n",
    "            The output size of the layer\n",
    "        \"\"\"\n",
    "        return self.layers[layer_id][0].out_features\n",
    "\n",
    "    def forward(self, x, circuit=None, interventions=None, return_activations=False):\n",
    "        \"\"\"\n",
    "        Applies a forward pass through the model, optionally using only a subset of the model (circuit) and applying\n",
    "        interventions.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor\n",
    "            circuit: The circuit to apply\n",
    "            interventions: The list of interventions to apply\n",
    "            return_activations: Whether to return the activations of each layer\n",
    "\n",
    "        Returns:\n",
    "            The output of the model if return_activations is False, else the activations of each layer\n",
    "        \"\"\"\n",
    "        activations = [x.detach()] if return_activations else None\n",
    "\n",
    "        # Prepare the list of interventions, ordering it by layer\n",
    "        ordered_interventions = defaultdict(list)\n",
    "        if interventions is not None:\n",
    "            for (layer_idx, indices), values in interventions.items():\n",
    "                ordered_interventions[layer_idx].append((indices, values))\n",
    "\n",
    "        if 0 in ordered_interventions:\n",
    "            for indices, values in ordered_interventions[0]:\n",
    "                x[list(indices)] = values\n",
    "\n",
    "        original_weights = []  # Store original weights if a circuit is applied\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if circuit:\n",
    "                # Store original weights before applying circuit edge masks\n",
    "                original_weights.append(layer[0].weight.data.clone())\n",
    "                layer[0].weight.data *= torch.tensor(\n",
    "                    circuit.edge_masks[i], dtype=torch.float32, device=self.device\n",
    "                )\n",
    "                if circuit.node_masks:\n",
    "                    x *= torch.tensor(\n",
    "                        circuit.node_masks[i], dtype=torch.float32, device=self.device\n",
    "                    )\n",
    "\n",
    "            # Forward pass\n",
    "            x = layer(x)\n",
    "\n",
    "            if return_activations:\n",
    "                activations.append(x.detach())\n",
    "\n",
    "            # Apply interventions at the current layer\n",
    "            if interventions and i + 1 in ordered_interventions:\n",
    "                for indices, values in ordered_interventions[i + 1]:\n",
    "                    x[list(indices)] = values\n",
    "\n",
    "        if circuit:\n",
    "            # Restore original weights\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                layer[0].weight.data = original_weights[i]\n",
    "\n",
    "        return activations if return_activations else x\n",
    "\n",
    "    def get_states(self, x, states: Dict[Any, Any]) -> Dict[Any, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Runs an input through the model and returns selected hidden states.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor\n",
    "            states: A dictionary of tuples (layer, indices) to return\n",
    "\n",
    "        Returns:\n",
    "            A dictionary mapping each tuple (layer, indices) to the corresponding hidden states\n",
    "        \"\"\"\n",
    "        activations = self(x, return_activations=True)\n",
    "        return {\n",
    "            (layer, tuple(indices)): activations[layer][indices]\n",
    "            for layer, indices in states.items()\n",
    "        }\n",
    "\n",
    "    def visualize(self, ax=None, circuit=None, activations=None):\n",
    "        \"\"\"\n",
    "        Visualize the model's structure or activations\n",
    "\n",
    "        Args:\n",
    "            ax: The axis to plot on\n",
    "            circuit: The optional circuit to visualize\n",
    "            activations: The optional activations to visualize\n",
    "        \"\"\"\n",
    "\n",
    "        G = nx.DiGraph()\n",
    "        pos = {}\n",
    "        colors = {}\n",
    "        node_labels = {}\n",
    "        edge_labels = {}\n",
    "        use_activation_values = True\n",
    "\n",
    "        # Set default values\n",
    "        if circuit is None:\n",
    "            from .circuit import Circuit\n",
    "\n",
    "            circuit = Circuit.full(self.layer_sizes)\n",
    "        node_masks = circuit.node_masks\n",
    "        edge_masks = circuit.edge_masks\n",
    "\n",
    "        if activations is None:\n",
    "            activations = [torch.ones(1, size) for size in self.layer_sizes]\n",
    "            use_activation_values = False\n",
    "\n",
    "        # Build the graph\n",
    "        max_width = max(self.layer_sizes)\n",
    "\n",
    "        for layer_idx, layer_activations in enumerate(activations):\n",
    "            num_nodes = layer_activations.shape[-1]\n",
    "            y_start = -(max_width - num_nodes) / 2  # Center nodes vertically\n",
    "\n",
    "            for node_idx in range(num_nodes):\n",
    "                node_id = f\"({layer_idx},{node_idx})\"\n",
    "                G.add_node(node_id)\n",
    "                pos[node_id] = (layer_idx, y_start - node_idx)\n",
    "\n",
    "                # Apply node mask for visualization\n",
    "                if node_masks is not None:\n",
    "                    active = node_masks[layer_idx][node_idx].item() == 1\n",
    "                else:\n",
    "                    active = True\n",
    "\n",
    "                if use_activation_values:\n",
    "                    activation_value = layer_activations[0, node_idx].item()\n",
    "                    node_labels[node_id] = f\"{activation_value:.2f}\"\n",
    "                    color_intensity = np.clip(activation_value, 0, 1)  # Normalize to [0, 1]\n",
    "                    node_color = plt.cm.YlOrRd(color_intensity) if active else 'grey'  # Use heatmap for activations\n",
    "                else:\n",
    "                    node_color = 'tab:blue' if active else 'grey'\n",
    "                colors[node_id] = node_color\n",
    "\n",
    "        # Add edges to the graph and create edge labels\n",
    "        for layer_idx, edge_mask in enumerate(edge_masks):\n",
    "            for out_idx, row in enumerate(edge_mask):\n",
    "                for in_idx, active in enumerate(row):\n",
    "                    from_node_id = f\"({layer_idx},{in_idx})\"\n",
    "                    to_node_id = f\"({layer_idx + 1},{out_idx})\"\n",
    "                    G.add_edge(from_node_id, to_node_id, active=active.item())\n",
    "\n",
    "                    weight = self.layers[layer_idx][0].weight[out_idx, in_idx].item()\n",
    "                    edge_labels[(from_node_id, to_node_id)] = f\"{weight:.2f}\"\n",
    "\n",
    "        # Draw nodes with color corresponding to activation value\n",
    "        node_colors = [colors[node] for node in G.nodes]\n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=1400, alpha=0.8)\n",
    "\n",
    "        # Draw edges\n",
    "        active_edges = [(u, v) for u, v, attr in G.edges(data=True) if attr['active'] == 1]\n",
    "        inactive_edges = [(u, v) for u, v, attr in G.edges(data=True) if attr['active'] == 0]\n",
    "        nx.draw_networkx_edges(G, pos, node_size=1400, edgelist=active_edges,\n",
    "                               edge_color='tab:red' if use_activation_values else 'tab:blue', width=1, alpha=0.6,\n",
    "                               arrows=True, arrowstyle=\"-|>\", connectionstyle='arc3,rad=0.1', ax=ax)\n",
    "        nx.draw_networkx_edges(G, pos, node_size=1400, edgelist=inactive_edges, edge_color='grey', width=1, alpha=0.5,\n",
    "                               style='dashed', arrows=True, arrowstyle=\"-|>\", connectionstyle='arc3,rad=0.1', ax=ax)\n",
    "\n",
    "        # Draw node labels (activation values)\n",
    "        nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=15, font_color='black', alpha=0.8, ax=ax)\n",
    "\n",
    "        # Draw edge labels (weights)\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=7, alpha=0.7, label_pos=0.6, ax=ax)\n",
    "\n",
    "        if use_activation_values:\n",
    "            plt.title(\"Neural Network Execution Visualization\")\n",
    "        else:\n",
    "            plt.title(\"Neural Network Visualization\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def __getitem__(self, idx, in_place=False):\n",
    "        \"\"\"\n",
    "        Overload the indexing operator to create a submodel\n",
    "\n",
    "        Args:\n",
    "            idx: The index or slice to use for the submodel\n",
    "            in_place: Whether to modify the original model weights in-place\n",
    "\n",
    "        Returns:\n",
    "            A new submodel, with shared weights if in_place is True, or a copy otherwise\n",
    "        \"\"\"\n",
    "        if isinstance(idx, slice):\n",
    "            # Handle negative indices in the slice\n",
    "            start_idx = idx.start if idx.start is not None else 0\n",
    "            stop_idx = idx.stop if idx.stop is not None else len(self.layers)\n",
    "            step_idx = idx.step if idx.step is not None else 1\n",
    "\n",
    "            # Adjust negative indices\n",
    "            if start_idx < 0:\n",
    "                start_idx += len(self.layers)\n",
    "            if stop_idx < 0:\n",
    "                stop_idx += len(self.layers)\n",
    "\n",
    "            # Extract the sliced layers\n",
    "            sliced_layers = self.layers[start_idx:stop_idx:step_idx]\n",
    "            num_sliced_layers = len(sliced_layers)\n",
    "\n",
    "            # Calculate the input size for the submodel (first layer in the slice)\n",
    "            input_size = self.layer_sizes[start_idx]\n",
    "\n",
    "            # Calculate the hidden sizes and output size for the submodel\n",
    "            smaller_hidden_sizes = [layer[0].out_features for layer in sliced_layers[:-1]]\n",
    "            output_size = sliced_layers[-1][0].out_features\n",
    "\n",
    "            # Create a new submodel with the correct architecture\n",
    "            submodel = MLP(\n",
    "                hidden_sizes=smaller_hidden_sizes,\n",
    "                input_size=input_size,\n",
    "                output_size=output_size,\n",
    "                activation=self.activation,\n",
    "                device=self.device,\n",
    "            )\n",
    "\n",
    "            # Copy weights and biases from the original model\n",
    "            for i, layer in enumerate(sliced_layers):\n",
    "                layer_data = submodel.layers[i][0]\n",
    "                layer_data.weight.data = layer[0].weight.data\n",
    "                layer_data.bias.data = layer[0].bias.data\n",
    "                if not in_place:\n",
    "                    layer_data.weight.data = layer_data.weight.data.clone()\n",
    "                    layer_data.bias.data = layer_data.bias.data.clone()\n",
    "\n",
    "            return submodel\n",
    "        else:\n",
    "            # Handle single index\n",
    "            if in_place:\n",
    "                return self.layers[idx]\n",
    "            return copy.deepcopy(self.layers[idx])\n",
    "\n",
    "    def do_train(self, x, y, x_val, y_val, batch_size, learning_rate, epochs, loss_target=0.001, val_frequency=10,\n",
    "                 early_stopping_steps=3, logger=None, l1_lambda=0.0, l2_lambda=0.0):\n",
    "        \"\"\"\n",
    "        Train the model using the given data and hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            x: The training input tensor\n",
    "            y: The training target tensor\n",
    "            x_val: The validation input tensor\n",
    "            y_val: The validation target tensor\n",
    "            batch_size: The batch size for training\n",
    "            learning_rate: The learning rate for training\n",
    "            epochs: The number of epochs to train\n",
    "            loss_target: The target loss value to stop training\n",
    "            val_frequency: The frequency of validation during training\n",
    "            early_stopping_steps: The number of epochs without improvement to stop training\n",
    "            logger: The logger to log training progress\n",
    "            l1_lambda: L1 regularization coefficient (0.0 = no L1)\n",
    "            l2_lambda: L2 regularization coefficient (0.0 = no L2)\n",
    "\n",
    "        Returns:\n",
    "            The average loss after training\n",
    "        \"\"\"\n",
    "        # Create a DataLoader for the training dataset\n",
    "        dataset = TensorDataset(x, y)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        bad_epochs = 0\n",
    "\n",
    "        val_acc = 0\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            epoch_loss = []\n",
    "            for inputs, targets in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                \n",
    "                # Base loss\n",
    "                base_loss = criterion(outputs, targets)\n",
    "                loss = base_loss\n",
    "                \n",
    "                # Add L1 Regularization\n",
    "                if l1_lambda > 0:\n",
    "                    l1_penalty = sum(torch.sum(torch.abs(param)) for param in self.parameters())\n",
    "                    loss = loss + l1_lambda * l1_penalty\n",
    "                \n",
    "                # Add L2 Regularization\n",
    "                if l2_lambda > 0:\n",
    "                    l2_penalty = sum(torch.sum(param ** 2) for param in self.parameters())\n",
    "                    loss = loss + l2_lambda * l2_penalty\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss.append(loss.item())\n",
    "\n",
    "            avg_loss = np.mean(epoch_loss)\n",
    "\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                bad_epochs = 0\n",
    "            else:\n",
    "                bad_epochs += 1\n",
    "\n",
    "            # Early stopping\n",
    "            if avg_loss < loss_target or bad_epochs >= early_stopping_steps:\n",
    "                break\n",
    "\n",
    "            # Print training progress\n",
    "            if (epoch + 1) % val_frequency == 0 and logger is not None:\n",
    "                self.eval()\n",
    "                with torch.no_grad():\n",
    "                    train_outputs = self(x)\n",
    "                    train_predictions = torch.round(train_outputs)\n",
    "                    correct_predictions_train = train_predictions.eq(y).all(dim=1)\n",
    "                    train_acc = correct_predictions_train.sum().item() / y.size(0)\n",
    "\n",
    "                    val_outputs = self(x_val)\n",
    "                    val_loss = criterion(val_outputs, y_val).item()\n",
    "                    val_predictions = torch.round(val_outputs)\n",
    "                    correct_predictions_val = val_predictions.eq(y_val).all(dim=1)\n",
    "                    val_acc = correct_predictions_val.sum().item() / y_val.size(0)\n",
    "\n",
    "                    # Log regularization info\n",
    "                    reg_info = \"\"\n",
    "                    if l1_lambda > 0:\n",
    "                        reg_info += f\", L1: {l1_lambda}\"\n",
    "                    if l2_lambda > 0:\n",
    "                        reg_info += f\", L2: {l2_lambda}\"\n",
    "                    if self.dropout_rate > 0:\n",
    "                        reg_info += f\", Dropout: {self.dropout_rate}\"\n",
    "\n",
    "                    logger.info(f'Epoch [{epoch + 1}/{epochs}], '\n",
    "                                f'Train Loss: {avg_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "                    logger.info(f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}, Bad Epochs: {bad_epochs}{reg_info}')\n",
    "\n",
    "        return avg_loss\n",
    "\n",
    "    def do_eval(self, x_test, y_test):\n",
    "        \"\"\"\n",
    "        Performs evaluation on the given test data\n",
    "\n",
    "        Args:\n",
    "            x_test: The test input tensor\n",
    "            y_test: The test target tensor\n",
    "\n",
    "        Returns:\n",
    "            The accuracy of the model on the test data\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = self(x_test)\n",
    "            val_predictions = torch.round(val_outputs)\n",
    "            correct_predictions_val = val_predictions.eq(y_test).all(dim=1)\n",
    "            acc = correct_predictions_val.sum().item() / y_test.size(0)\n",
    "        return acc\n",
    "\n",
    "    def separate_into_k_mlps(self):\n",
    "        \"\"\"\n",
    "        Separates the original MLP into K individual MLPs, each with output size 1.\n",
    "\n",
    "        Returns:\n",
    "            A list of K MLP models, each for a single output.\n",
    "        \"\"\"\n",
    "        separate_models = [self[:] for _ in range(self.output_size)]\n",
    "\n",
    "        last_layer = self.layers[-1]\n",
    "        last_layer_weights = last_layer[0].weight.data\n",
    "        last_layer_biases = last_layer[0].bias.data\n",
    "\n",
    "        for i, model in enumerate(separate_models):\n",
    "            model.layers[-1][0] = nn.Linear(self.hidden_sizes[-1], 1)\n",
    "            model.layers[-1][0].weight.data = last_layer_weights[i:i + 1, :].clone()\n",
    "            model.layers[-1][0].bias.data = last_layer_biases[i:i + 1].clone()\n",
    "            model.output_size = 1\n",
    "            model.layer_sizes = model.layer_sizes[:-1] + [1]\n",
    "\n",
    "        return separate_models\n",
    "\n",
    "    def enumerate_valid_node_masks(self):\n",
    "        \"\"\"\n",
    "        Generate all valid node masks in the neural network.\n",
    "\n",
    "        Returns:\n",
    "            A list of all valid node masks\n",
    "        \"\"\"\n",
    "        # Generate all valid masks for each layer\n",
    "        all_masks_per_layer = []\n",
    "\n",
    "        for size in self.layer_sizes[1:]:\n",
    "            masks = [np.array([int(x) for x in format(i, f'0{size}b')]) for i in range(2 ** size)]\n",
    "            all_masks_per_layer.append(masks)\n",
    "\n",
    "        # Generate combinations of masks for all layers\n",
    "        all_masks = list(itertools.product(*all_masks_per_layer))\n",
    "        return all_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "main-modifications"
   },
   "source": [
    "Now let's modify the main.py file to accept regularization parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "patch-main"
   },
   "outputs": [],
   "source": [
    "# Read the original main.py\n",
    "with open('main.py', 'r') as f:\n",
    "    main_content = f.read()\n",
    "\n",
    "# Add regularization arguments to argparse section\n",
    "import_section = main_content.split('if __name__ == \"__main__\":')[0]\n",
    "main_section = main_content.split('if __name__ == \"__main__\":')[1]\n",
    "\n",
    "# Find where to insert new arguments (after --verbose)\n",
    "insert_pos = main_section.find(\"parser.add_argument('--resume-from'\")\n",
    "\n",
    "new_args = '''\n",
    "    parser.add_argument('--l1-lambda', type=float, default=0.0,\n",
    "                        help='L1 regularization coefficient (default: 0.0)')\n",
    "    parser.add_argument('--l2-lambda', type=float, default=0.0,\n",
    "                        help='L2 regularization coefficient (default: 0.0)')\n",
    "    parser.add_argument('--dropout-rate', type=float, default=0.0,\n",
    "                        help='Dropout rate, 0.0 to 1.0 (default: 0.0)')\n",
    "    '''\n",
    "\n",
    "main_section = main_section[:insert_pos] + new_args + main_section[insert_pos:]\n",
    "\n",
    "# Modify model creation to include dropout\n",
    "main_section = main_section.replace(\n",
    "    'model = MLP(hidden_sizes=layer_sizes[1:-1], input_size=n_inputs, output_size=n_gates, device=args.device)',\n",
    "    'model = MLP(hidden_sizes=layer_sizes[1:-1], input_size=n_inputs, output_size=n_gates, device=args.device, dropout_rate=args.dropout_rate)'\n",
    ")\n",
    "\n",
    "# Modify training call to include regularization\n",
    "main_section = main_section.replace(\n",
    "    '''avg_loss = model.do_train(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                x_val=x_val,\n",
    "                y_val=y_val,\n",
    "                batch_size=args.batch_size,\n",
    "                learning_rate=lr,\n",
    "                epochs=args.epochs,\n",
    "                loss_target=loss_target,\n",
    "                val_frequency=args.val_frequency,\n",
    "                logger=logger if args.verbose else None\n",
    "            )''',\n",
    "    '''avg_loss = model.do_train(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                x_val=x_val,\n",
    "                y_val=y_val,\n",
    "                batch_size=args.batch_size,\n",
    "                learning_rate=lr,\n",
    "                epochs=args.epochs,\n",
    "                loss_target=loss_target,\n",
    "                val_frequency=args.val_frequency,\n",
    "                logger=logger if args.verbose else None,\n",
    "                l1_lambda=args.l1_lambda,\n",
    "                l2_lambda=args.l2_lambda\n",
    "            )'''\n",
    ")\n",
    "\n",
    "# Add regularization info to saved data\n",
    "main_section = main_section.replace(\n",
    "    \"'weights': weights,\",\n",
    "    ''''weights': weights,\n",
    "                'l1_lambda': args.l1_lambda,\n",
    "                'l2_lambda': args.l2_lambda,\n",
    "                'dropout_rate': args.dropout_rate'''\n",
    ")\n",
    "\n",
    "# Write modified main.py\n",
    "with open('main.py', 'w') as f:\n",
    "    f.write(import_section + 'if __name__ == \"__main__\":' + main_section)\n",
    "\n",
    "print(\"✓ main.py modified successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "experiment-header"
   },
   "source": [
    "## Run Experiments\n",
    "\n",
    "Now we're ready to run experiments! Let's start with a quick test to make sure everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-run"
   },
   "outputs": [],
   "source": [
    "# Quick test run with baseline (no regularization)\n",
    "!python main.py --verbose --val-frequency 1 --noise-std 0.0 --target-logic-gates XOR \\\n",
    "  --n-experiments 3 --size 3 --depth 2 --device {device}\n",
    "\n",
    "print(\"\\n✓ Test run completed! Now you can run full experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baseline-header"
   },
   "source": [
    "### Baseline Experiment (No Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-baseline"
   },
   "outputs": [],
   "source": [
    "# Baseline - 30 experiments (reduced from 100 for Colab)\n",
    "!python main.py --verbose --val-frequency 1 --noise-std 0.0 --target-logic-gates XOR \\\n",
    "  --n-experiments 30 --size 3 --depth 2 --device {device}\n",
    "\n",
    "print(\"\\n✓ Baseline experiment completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1-header"
   },
   "source": [
    "### L1 Regularization Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-l1"
   },
   "outputs": [],
   "source": [
    "# L1 Experiments\n",
    "l1_lambdas = [0.0001, 0.001, 0.01]\n",
    "\n",
    "for l1 in l1_lambdas:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running L1 experiment with lambda={l1}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    !python main.py --verbose --val-frequency 1 --noise-std 0.0 --target-logic-gates XOR \\\n",
    "      --n-experiments 30 --size 3 --depth 2 --device {device} --l1-lambda {l1}\n",
    "\n",
    "print(\"\\n✓ All L1 experiments completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2-header"
   },
   "source": [
    "### L2 Regularization Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-l2"
   },
   "outputs": [],
   "source": [
    "# L2 Experiments\n",
    "l2_lambdas = [0.0001, 0.001, 0.01]\n",
    "\n",
    "for l2 in l2_lambdas:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running L2 experiment with lambda={l2}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    !python main.py --verbose --val-frequency 1 --noise-std 0.0 --target-logic-gates XOR \\\n",
    "      --n-experiments 30 --size 3 --depth 2 --device {device} --l2-lambda {l2}\n",
    "\n",
    "print(\"\\n✓ All L2 experiments completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dropout-header"
   },
   "source": [
    "### Dropout Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-dropout"
   },
   "outputs": [],
   "source": [
    "# Dropout Experiments\n",
    "dropout_rates = [0.1, 0.3, 0.5]\n",
    "\n",
    "for dr in dropout_rates:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running Dropout experiment with rate={dr}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    !python main.py --verbose --val-frequency 1 --noise-std 0.0 --target-logic-gates XOR \\\n",
    "      --n-experiments 30 --size 3 --depth 2 --device {device} --dropout-rate {dr}\n",
    "\n",
    "print(\"\\n✓ All Dropout experiments completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "analysis-header"
   },
   "source": [
    "## Analysis\n",
    "\n",
    "Now let's analyze the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-results"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Load all results\n",
    "results_dir = Path('logs')\n",
    "all_data = []\n",
    "\n",
    "for csv_file in results_dir.glob(\"**/df_out.csv\"):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    all_data.append(df)\n",
    "\n",
    "if all_data:\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    print(f\"Loaded {len(combined_df)} experiment results\")\n",
    "    print(f\"\\nColumns: {combined_df.columns.tolist()}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(combined_df.head())\n",
    "else:\n",
    "    print(\"No results found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "preprocess-data"
   },
   "outputs": [],
   "source": [
    "# Preprocess data - extract circuit counts from lists\n",
    "import ast\n",
    "\n",
    "def extract_first(x):\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            x = ast.literal_eval(x)\n",
    "        except:\n",
    "            return x\n",
    "    if isinstance(x, list) and len(x) > 0:\n",
    "        return x[0]\n",
    "    return x\n",
    "\n",
    "combined_df['n_circuits'] = combined_df['perfect_circuits'].apply(extract_first)\n",
    "combined_df['n_formulas'] = combined_df['formulas'].apply(extract_first)\n",
    "\n",
    "# Ensure regularization columns exist with defaults\n",
    "if 'l1_lambda' not in combined_df.columns:\n",
    "    combined_df['l1_lambda'] = 0.0\n",
    "if 'l2_lambda' not in combined_df.columns:\n",
    "    combined_df['l2_lambda'] = 0.0\n",
    "if 'dropout_rate' not in combined_df.columns:\n",
    "    combined_df['dropout_rate'] = 0.0\n",
    "\n",
    "print(\"Data preprocessed!\")\n",
    "print(f\"\\nCircuit count statistics:\")\n",
    "print(combined_df['n_circuits'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot-l1"
   },
   "outputs": [],
   "source": [
    "# Plot L1 effects\n",
    "if combined_df['l1_lambda'].nunique() > 1:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Box plot\n",
    "    sns.boxplot(data=combined_df, x='l1_lambda', y='n_circuits', ax=axes[0])\n",
    "    axes[0].set_title('L1 Regularization: Effect on Circuit Count', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('L1 Lambda', fontsize=12)\n",
    "    axes[0].set_ylabel('Number of Circuits', fontsize=12)\n",
    "    \n",
    "    # Mean trend\n",
    "    mean_circuits = combined_df.groupby('l1_lambda')['n_circuits'].mean()\n",
    "    axes[1].plot(mean_circuits.index, mean_circuits.values, marker='o', linewidth=2, markersize=8)\n",
    "    axes[1].set_title('L1 Regularization: Mean Circuit Count', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('L1 Lambda', fontsize=12)\n",
    "    axes[1].set_ylabel('Mean Circuit Count', fontsize=12)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('l1_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical test\n",
    "    baseline = combined_df[combined_df['l1_lambda'] == 0.0]['n_circuits']\n",
    "    for l1 in combined_df['l1_lambda'].unique():\n",
    "        if l1 > 0:\n",
    "            treatment = combined_df[combined_df['l1_lambda'] == l1]['n_circuits']\n",
    "            t_stat, p_val = stats.ttest_ind(baseline, treatment)\n",
    "            print(f\"L1={l1}: Mean change = {treatment.mean() - baseline.mean():.2f}, p={p_val:.4f}\")\n",
    "else:\n",
    "    print(\"L1 experiments not found in data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot-l2"
   },
   "outputs": [],
   "source": [
    "# Plot L2 effects\n",
    "if combined_df['l2_lambda'].nunique() > 1:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Box plot\n",
    "    sns.boxplot(data=combined_df, x='l2_lambda', y='n_circuits', ax=axes[0])\n",
    "    axes[0].set_title('L2 Regularization: Effect on Circuit Count', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('L2 Lambda', fontsize=12)\n",
    "    axes[0].set_ylabel('Number of Circuits', fontsize=12)\n",
    "    \n",
    "    # Mean trend\n",
    "    mean_circuits = combined_df.groupby('l2_lambda')['n_circuits'].mean()\n",
    "    axes[1].plot(mean_circuits.index, mean_circuits.values, marker='o', linewidth=2, markersize=8, color='orange')\n",
    "    axes[1].set_title('L2 Regularization: Mean Circuit Count', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('L2 Lambda', fontsize=12)\n",
    "    axes[1].set_ylabel('Mean Circuit Count', fontsize=12)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('l2_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical test\n",
    "    baseline = combined_df[combined_df['l2_lambda'] == 0.0]['n_circuits']\n",
    "    for l2 in combined_df['l2_lambda'].unique():\n",
    "        if l2 > 0:\n",
    "            treatment = combined_df[combined_df['l2_lambda'] == l2]['n_circuits']\n",
    "            t_stat, p_val = stats.ttest_ind(baseline, treatment)\n",
    "            print(f\"L2={l2}: Mean change = {treatment.mean() - baseline.mean():.2f}, p={p_val:.4f}\")\n",
    "else:\n",
    "    print(\"L2 experiments not found in data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot-dropout"
   },
   "outputs": [],
   "source": [
    "# Plot Dropout effects\n",
    "if combined_df['dropout_rate'].nunique() > 1:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Box plot\n",
    "    sns.boxplot(data=combined_df, x='dropout_rate', y='n_circuits', ax=axes[0])\n",
    "    axes[0].set_title('Dropout: Effect on Circuit Count', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Dropout Rate', fontsize=12)\n",
    "    axes[0].set_ylabel('Number of Circuits', fontsize=12)\n",
    "    \n",
    "    # Mean trend\n",
    "    mean_circuits = combined_df.groupby('dropout_rate')['n_circuits'].mean()\n",
    "    axes[1].plot(mean_circuits.index, mean_circuits.values, marker='o', linewidth=2, markersize=8, color='green')\n",
    "    axes[1].set_title('Dropout: Mean Circuit Count', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Dropout Rate', fontsize=12)\n",
    "    axes[1].set_ylabel('Mean Circuit Count', fontsize=12)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dropout_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical test\n",
    "    baseline = combined_df[combined_df['dropout_rate'] == 0.0]['n_circuits']\n",
    "    for dr in combined_df['dropout_rate'].unique():\n",
    "        if dr > 0:\n",
    "            treatment = combined_df[combined_df['dropout_rate'] == dr]['n_circuits']\n",
    "            t_stat, p_val = stats.ttest_ind(baseline, treatment)\n",
    "            print(f\"Dropout={dr}: Mean change = {treatment.mean() - baseline.mean():.2f}, p={p_val:.4f}\")\n",
    "else:\n",
    "    print(\"Dropout experiments not found in data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "summary-stats"
   },
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: REGULARIZATION EFFECTS ON PARALLEL CIRCUITS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "baseline_circuits = combined_df[\n",
    "    (combined_df['l1_lambda'] == 0) & \n",
    "    (combined_df['l2_lambda'] == 0) & \n",
    "    (combined_df['dropout_rate'] == 0)\n",
    "]['n_circuits']\n",
    "\n",
    "if len(baseline_circuits) > 0:\n",
    "    print(f\"Baseline (no regularization):\")\n",
    "    print(f\"  Mean circuits: {baseline_circuits.mean():.2f} ± {baseline_circuits.std():.2f}\")\n",
    "    print(f\"  Median: {baseline_circuits.median():.0f}\")\n",
    "    print(f\"  Range: [{baseline_circuits.min():.0f}, {baseline_circuits.max():.0f}]\")\n",
    "    print()\n",
    "\n",
    "# Check hypotheses\n",
    "print(\"HYPOTHESIS TESTING:\\n\")\n",
    "\n",
    "if combined_df['l1_lambda'].nunique() > 1:\n",
    "    l1_max = combined_df[combined_df['l1_lambda'] > 0]['l1_lambda'].max()\n",
    "    l1_circuits = combined_df[combined_df['l1_lambda'] == l1_max]['n_circuits']\n",
    "    l1_change = l1_circuits.mean() - baseline_circuits.mean()\n",
    "    print(f\"L1 Hypothesis (sparsity → fewer circuits):\")\n",
    "    print(f\"  Strongest L1={l1_max}: {l1_change:+.2f} circuits\")\n",
    "    print(f\"  Direction: {'✓ SUPPORTED' if l1_change < 0 else '✗ NOT SUPPORTED'}\")\n",
    "    print()\n",
    "\n",
    "if combined_df['l2_lambda'].nunique() > 1:\n",
    "    l2_max = combined_df[combined_df['l2_lambda'] > 0]['l2_lambda'].max()\n",
    "    l2_circuits = combined_df[combined_df['l2_lambda'] == l2_max]['n_circuits']\n",
    "    l2_change = l2_circuits.mean() - baseline_circuits.mean()\n",
    "    print(f\"L2 Hypothesis (diffusion → more circuits):\")\n",
    "    print(f\"  Strongest L2={l2_max}: {l2_change:+.2f} circuits\")\n",
    "    print(f\"  Direction: {'✓ SUPPORTED' if l2_change > 0 else '✗ NOT SUPPORTED'}\")\n",
    "    print()\n",
    "\n",
    "if combined_df['dropout_rate'].nunique() > 1:\n",
    "    dr_max = combined_df[combined_df['dropout_rate'] > 0]['dropout_rate'].max()\n",
    "    dr_circuits = combined_df[combined_df['dropout_rate'] == dr_max]['n_circuits']\n",
    "    dr_change = dr_circuits.mean() - baseline_circuits.mean()\n",
    "    print(f\"Dropout Hypothesis (redundancy → more circuits):\")\n",
    "    print(f\"  Strongest dropout={dr_max}: {dr_change:+.2f} circuits\")\n",
    "    print(f\"  Direction: {'✓ SUPPORTED' if dr_change > 0 else '✗ NOT SUPPORTED'}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export-header"
   },
   "source": [
    "## Export Results\n",
    "\n",
    "Let's download the results and plots to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export-results"
   },
   "outputs": [],
   "source": [
    "# Save combined results\n",
    "combined_df.to_csv('all_results.csv', index=False)\n",
    "print(\"Results saved to all_results.csv\")\n",
    "\n",
    "# Create a zip file with all results\n",
    "!zip -r regularization_results.zip logs/ *.png all_results.csv\n",
    "\n",
    "print(\"\\nAll results packaged in regularization_results.zip\")\n",
    "print(\"Download this file to your local machine!\")\n",
    "\n",
    "# In Colab, you can download with:\n",
    "from google.colab import files\n",
    "files.download('regularization_results.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next-steps"
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "### If you want to run more experiments:\n",
    "\n",
    "1. **Test combined regularization:**\n",
    "   ```python\n",
    "   !python main.py --verbose --val-frequency 1 --noise-std 0.0 --target-logic-gates XOR \\\n",
    "     --n-experiments 30 --size 3 --depth 2 --device {device} \\\n",
    "     --l1-lambda 0.001 --l2-lambda 0.001 --dropout-rate 0.3\n",
    "   ```\n",
    "\n",
    "2. **Test on different tasks:**\n",
    "   ```python\n",
    "   !python main.py --verbose --val-frequency 1 --noise-std 0.0 \\\n",
    "     --target-logic-gates AND OR IMP \\\n",
    "     --n-experiments 30 --size 3 --depth 2 --device {device} --l2-lambda 0.001\n",
    "   ```\n",
    "\n",
    "3. **Test on larger networks:**\n",
    "   ```python\n",
    "   !python main.py --verbose --val-frequency 1 --noise-std 0.0 --target-logic-gates XOR \\\n",
    "     --n-experiments 30 --size 5 --depth 3 --device {device} --dropout-rate 0.3\n",
    "   ```\n",
    "\n",
    "### To continue analysis:\n",
    "- Re-run the analysis cells after each new experiment\n",
    "- The plots will automatically update with new data\n",
    "- Download updated results using the export cell\n",
    "\n",
    "### Remember:\n",
    "- Colab sessions timeout after 12 hours of inactivity\n",
    "- Download your results regularly!\n",
    "- GPU availability may be limited on free tier"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Regularization Effects on Parallel Circuits (Fast Version)\n",
    "\n",
    "This notebook implements experiments to test how L1, L2, and dropout regularization affect parallel circuit emergence in neural networks.\n",
    "\n",
    "**Hypotheses:**\n",
    "- **L1 regularization** → Sparsity → Fewer parallel circuits\n",
    "- **L2 regularization** → Weight diffusion → More parallel circuits  \n",
    "- **Dropout** → Forced redundancy → More parallel circuits\n",
    "\n",
    "**Note:** This version removes the slow grounding/interpretation calculations and focuses only on counting circuits (neuron combinations).\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    print(\"Warning: CUDA not available, using CPU\")\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "# Clone the MI-identifiability repository\n",
    "!git clone https://github.com/MelouxM/MI-identifiability.git\n",
    "%cd MI-identifiability\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "modifications-header"
   },
   "source": [
    "## Code Modifications\n",
    "\n",
    "We'll modify the code to:\n",
    "1. Add regularization support (L1, L2, dropout)\n",
    "2. Remove slow grounding/interpretation calculations\n",
    "3. Keep only circuit counting (what you care about!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "backup-original"
   },
   "outputs": [],
   "source": [
    "# Backup original files\n",
    "!cp mi_identifiability/neural_model.py mi_identifiability/neural_model.py.bak\n",
    "!cp main.py main.py.bak\n",
    "print(\"Original files backed up!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "modify-main-fast"
   },
   "outputs": [],
   "source": [
    "# Modify main.py to remove grounding and add regularization\n",
    "\n",
    "with open('main.py', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Step 1: Add regularization arguments\n",
    "import_section = content.split('if __name__ == \"__main__\":')[0]\n",
    "main_section = content.split('if __name__ == \"__main__\":')[1]\n",
    "\n",
    "# Find where to insert new arguments\n",
    "insert_pos = main_section.find(\"parser.add_argument('--resume-from'\")\n",
    "\n",
    "new_args = '''parser.add_argument('--l1-lambda', type=float, default=0.0,\n",
    "                        help='L1 regularization coefficient (default: 0.0)')\n",
    "    parser.add_argument('--l2-lambda', type=float, default=0.0,\n",
    "                        help='L2 regularization coefficient (default: 0.0)')\n",
    "    parser.add_argument('--dropout-rate', type=float, default=0.0,\n",
    "                        help='Dropout rate, 0.0 to 1.0 (default: 0.0)')\n",
    "    '''\n",
    "\n",
    "main_section = main_section[:insert_pos] + new_args + main_section[insert_pos:]\n",
    "\n",
    "# Step 2: Modify model creation\n",
    "main_section = main_section.replace(\n",
    "    'model = MLP(hidden_sizes=layer_sizes[1:-1], input_size=n_inputs, output_size=n_gates, device=args.device)',\n",
    "    'model = MLP(hidden_sizes=layer_sizes[1:-1], input_size=n_inputs, output_size=n_gates, device=args.device, dropout_rate=args.dropout_rate)'\n",
    ")\n",
    "\n",
    "# Step 3: Modify training call\n",
    "main_section = main_section.replace(\n",
    "    '''avg_loss = model.do_train(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                x_val=x_val,\n",
    "                y_val=y_val,\n",
    "                batch_size=args.batch_size,\n",
    "                learning_rate=lr,\n",
    "                epochs=args.epochs,\n",
    "                loss_target=loss_target,\n",
    "                val_frequency=args.val_frequency,\n",
    "                logger=logger if args.verbose else None\n",
    "            )''',\n",
    "    '''avg_loss = model.do_train(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                x_val=x_val,\n",
    "                y_val=y_val,\n",
    "                batch_size=args.batch_size,\n",
    "                learning_rate=lr,\n",
    "                epochs=args.epochs,\n",
    "                loss_target=loss_target,\n",
    "                val_frequency=args.val_frequency,\n",
    "                logger=logger if args.verbose else None,\n",
    "                l1_lambda=args.l1_lambda,\n",
    "                l2_lambda=args.l2_lambda\n",
    "            )'''\n",
    ")\n",
    "\n",
    "# Step 4: REMOVE GROUNDING SECTION\n",
    "import re\n",
    "\n",
    "# Remove grounding loop (this is the slow part!)\n",
    "grounding_pattern = r'if args\\.max_circuits is not None and args\\.max_circuits < len\\(top_circuits\\):.*?print\\(f\\'Circuits: \\{n_circuits\\}, groundings: \\{total_n_groundings\\}\\'\\)'\n",
    "main_section = re.sub(grounding_pattern, \n",
    "                     '# REMOVED: Grounding calculations (not needed for circuit counting)',\n",
    "                     main_section, flags=re.DOTALL)\n",
    "\n",
    "# Remove formula/mapping section (also slow!)\n",
    "formula_pattern = r'formulas = set\\(\\).*?print\\(f\\'Formulas: \\{n_formulas\\}, mappings: \\{total_n_mappings\\}\\'\\)'\n",
    "main_section = re.sub(formula_pattern,\n",
    "                     '# REMOVED: Formula/mapping calculations (not needed for circuit counting)',\n",
    "                     main_section, flags=re.DOTALL)\n",
    "\n",
    "# Step 5: Simplify data storage (remove interpretation fields)\n",
    "main_section = main_section.replace(\n",
    "    \"'interpretations_per_circuit': total_n_groundings,\",\n",
    "    \"# 'interpretations_per_circuit' removed (not needed)\"\n",
    ")\n",
    "main_section = main_section.replace(\n",
    "    \"'formulas': n_formulas,\",\n",
    "    \"# 'formulas' removed (not needed)\"\n",
    ")\n",
    "main_section = main_section.replace(\n",
    "    \"'mappings_per_formula': total_n_mappings,\",\n",
    "    \"# 'mappings_per_formula' removed (not needed)\"\n",
    ")\n",
    "\n",
    "# Step 6: Add regularization info to data\n",
    "main_section = main_section.replace(\n",
    "    \"'weights': weights,\",\n",
    "    ''''weights': weights,\n",
    "                'l1_lambda': args.l1_lambda,\n",
    "                'l2_lambda': args.l2_lambda,\n",
    "                'dropout_rate': args.dropout_rate'''\n",
    ")\n",
    "\n",
    "# Write modified main.py\n",
    "with open('main.py', 'w') as f:\n",
    "    f.write(import_section + 'if __name__ == \"__main__\":' + main_section)\n",
    "\n",
    "print(\"✓ main.py modified successfully!\")\n",
    "print(\"  - Added regularization support\")\n",
    "print(\"  - Removed slow grounding calculations\")\n",
    "print(\"  - Removed formula/mapping calculations\")\n",
    "print(\"  - Kept circuit counting (what you need!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "modify-neural-model"
   },
   "outputs": [],
   "source": [
    "# Modify neural_model.py to add dropout and regularization\n",
    "# (This is the same as before - adds dropout support and L1/L2 regularization)\n",
    "\n",
    "with open('mi_identifiability/neural_model.py', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Add dropout_rate parameter to __init__\n",
    "content = content.replace(\n",
    "    'def __init__(self, hidden_sizes: list, input_size=2, output_size=1, activation=\\'leaky_relu\\', device=\\'cpu\\'):',\n",
    "    'def __init__(self, hidden_sizes: list, input_size=2, output_size=1, activation=\\'leaky_relu\\', device=\\'cpu\\', dropout_rate=0.0):'\n",
    ")\n",
    "\n",
    "# Store dropout_rate\n",
    "content = content.replace(\n",
    "    'self.activation = activation',\n",
    "    'self.activation = activation\\n        self.dropout_rate = dropout_rate'\n",
    ")\n",
    "\n",
    "# Modify layer creation to add dropout\n",
    "old_layers = '''self.layers = nn.ModuleList(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_size, out_size),\n",
    "                ACTIVATION_FUNCTIONS[activation]() if idx < len(self.layer_sizes) - 1 else nn.Identity()\n",
    "            ) for idx, (in_size, out_size) in enumerate(zip(self.layer_sizes, self.layer_sizes[1:]))\n",
    "        )'''\n",
    "\n",
    "new_layers = '''# Build layers with optional dropout\n",
    "        self.layers = nn.ModuleList()\n",
    "        for idx, (in_size, out_size) in enumerate(zip(self.layer_sizes, self.layer_sizes[1:])):\n",
    "            layer_components = [nn.Linear(in_size, out_size)]\n",
    "            \n",
    "            # Add dropout after linear layer (but not on output layer)\n",
    "            if idx < len(self.layer_sizes) - 2 and dropout_rate > 0:\n",
    "                layer_components.append(nn.Dropout(dropout_rate))\n",
    "            \n",
    "            # Add activation function (Identity for output layer)\n",
    "            if idx < len(self.layer_sizes) - 2:\n",
    "                layer_components.append(ACTIVATION_FUNCTIONS[activation]())\n",
    "            else:\n",
    "                layer_components.append(nn.Identity())\n",
    "            \n",
    "            self.layers.append(nn.Sequential(*layer_components))'''\n",
    "\n",
    "content = content.replace(old_layers, new_layers)\n",
    "\n",
    "# Add dropout_rate to save\n",
    "content = content.replace(\n",
    "    \"'activation': self.activation,\",\n",
    "    \"'activation': self.activation,\\n            'dropout_rate': self.dropout_rate,\"\n",
    ")\n",
    "\n",
    "# Add dropout_rate to load (with backwards compatibility)\n",
    "content = content.replace(\n",
    "    \"activation=model_data['activation']\",\n",
    "    \"activation=model_data['activation'],\\n            dropout_rate=model_data.get('dropout_rate', 0.0)\"\n",
    ")\n",
    "\n",
    "# Add L1 and L2 regularization to do_train\n",
    "content = content.replace(\n",
    "    'def do_train(self, x, y, x_val, y_val, batch_size, learning_rate, epochs, loss_target=0.001, val_frequency=10,\\n                 early_stopping_steps=3, logger=None):',\n",
    "    'def do_train(self, x, y, x_val, y_val, batch_size, learning_rate, epochs, loss_target=0.001, val_frequency=10,\\n                 early_stopping_steps=3, logger=None, l1_lambda=0.0, l2_lambda=0.0):'\n",
    ")\n",
    "\n",
    "# Add regularization to loss calculation\n",
    "old_loss = '''for inputs, targets in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()'''\n",
    "\n",
    "new_loss = '''for inputs, targets in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                \n",
    "                # Base loss\n",
    "                base_loss = criterion(outputs, targets)\n",
    "                loss = base_loss\n",
    "                \n",
    "                # Add L1 Regularization\n",
    "                if l1_lambda > 0:\n",
    "                    l1_penalty = sum(torch.sum(torch.abs(param)) for param in self.parameters())\n",
    "                    loss = loss + l1_lambda * l1_penalty\n",
    "                \n",
    "                # Add L2 Regularization\n",
    "                if l2_lambda > 0:\n",
    "                    l2_penalty = sum(torch.sum(param ** 2) for param in self.parameters())\n",
    "                    loss = loss + l2_lambda * l2_penalty\n",
    "                \n",
    "                loss.backward()'''\n",
    "\n",
    "content = content.replace(old_loss, new_loss)\n",
    "\n",
    "with open('mi_identifiability/neural_model.py', 'w') as f:\n",
    "    f.write(content)\n",
    "\n",
    "print(\"✓ neural_model.py modified successfully!\")\n",
    "print(\"  - Added dropout support\")\n",
    "print(\"  - Added L1/L2 regularization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-header"
   },
   "source": [
    "## Quick Test\n",
    "\n",
    "Let's verify everything works with a small test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-run"
   },
   "outputs": [],
   "source": [
    "# Quick test run with baseline (no regularization)\n",
    "!python main.py --verbose --val-frequency 1 --noise-std 0.0 --target-logic-gates XOR \\\n",
    "  --n-experiments 3 --size 3 --depth 2 --device {device}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Test run completed successfully!\")\n",
    "print(\"✓ Ready to run full experiments (much faster without grounding!)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "experiments-header"
   },
   "source": [
    "## Run Full Experiments\n",
    "\n",
    "Now we'll run the complete experimental suite. Each section can be run independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-baseline"
   },
   "outputs": [],
   "source": [
    "# Baseline - No regularization\n",
    "print(\"Running baseline experiments...\")\n",
    "!python main.py --verbose --val-frequency 1 --noise-std 0.0 --target-logic-gates XOR \\\n",
    "  --n-experiments 50 --size 3 --depth 2 --device {device}\n",
    "\n",
    "print(\"\\n✓ Baseline completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-l1"
   },
   "outputs": [],
   "source": [
    "# L1 Regularization Experiments\n",
    "l1_lambdas = [0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
    "\n",
    "for l1 in l1_lambdas:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running L1 experiment: lambda={l1}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    !python main.py --verbose --val-frequency 1 --noise-std 0.0 --target-logic-gates XOR \\\n",
    "      --n-experiments 50 --size 3 --depth 2 --device {device} --l1-lambda {l1}\n",
    "\n",
    "print(\"\\n✓ All L1 experiments completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-l2"
   },
   "outputs": [],
   "source": [
    "# L2 Regularization Experiments\n",
    "l2_lambdas = [0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
    "\n",
    "for l2 in l2_lambdas:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running L2 experiment: lambda={l2}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    !python main.py --verbose --val-frequency 1 --noise-std 0.0 --target-logic-gates XOR \\\n",
    "      --n-experiments 50 --size 3 --depth 2 --device {device} --l2-lambda {l2}\n",
    "\n",
    "print(\"\\n✓ All L2 experiments completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-dropout"
   },
   "outputs": [],
   "source": [
    "# Dropout Experiments\n",
    "dropout_rates = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "for dr in dropout_rates:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running Dropout experiment: rate={dr}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    !python main.py --verbose --val-frequency 1 --noise-std 0.0 --target-logic-gates XOR \\\n",
    "      --n-experiments 50 --size 3 --depth 2 --device {device} --dropout-rate {dr}\n",
    "\n",
    "print(\"\\n✓ All Dropout experiments completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "analysis-header"
   },
   "source": [
    "## Analysis\n",
    "\n",
    "Now let's analyze the results and test our hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-results"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "import ast\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Load all results\n",
    "results_dir = Path('logs')\n",
    "all_data = []\n",
    "\n",
    "for csv_file in results_dir.glob(\"**/df_out.csv\"):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    all_data.append(df)\n",
    "\n",
    "combined_df = pd.concat(all_data, ignore_index=True)\n",
    "print(f\"Loaded {len(combined_df)} experiment results\")\n",
    "\n",
    "# Extract circuit counts from lists\n",
    "def extract_first(x):\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            x = ast.literal_eval(x)\n",
    "        except:\n",
    "            return x\n",
    "    if isinstance(x, list) and len(x) > 0:\n",
    "        return x[0]\n",
    "    return x\n",
    "\n",
    "combined_df['n_circuits'] = combined_df['perfect_circuits'].apply(extract_first)\n",
    "\n",
    "# Ensure regularization columns exist\n",
    "for col in ['l1_lambda', 'l2_lambda', 'dropout_rate']:\n",
    "    if col not in combined_df.columns:\n",
    "        combined_df[col] = 0.0\n",
    "\n",
    "print(f\"\\nCircuit count summary:\")\n",
    "print(combined_df['n_circuits'].describe())\n",
    "print(f\"\\nRegularization conditions tested:\")\n",
    "print(f\"  L1 lambdas: {sorted(combined_df['l1_lambda'].unique())}\")\n",
    "print(f\"  L2 lambdas: {sorted(combined_df['l2_lambda'].unique())}\")\n",
    "print(f\"  Dropout rates: {sorted(combined_df['dropout_rate'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot-all"
   },
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Regularization Effects on Parallel Circuits', fontsize=16, fontweight='bold')\n",
    "\n",
    "# L1 - Box plot\n",
    "if combined_df['l1_lambda'].nunique() > 1:\n",
    "    sns.boxplot(data=combined_df, x='l1_lambda', y='n_circuits', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('L1: Circuit Count Distribution', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('L1 Lambda')\n",
    "    axes[0, 0].set_ylabel('Number of Circuits')\n",
    "\n",
    "# L1 - Mean trend\n",
    "if combined_df['l1_lambda'].nunique() > 1:\n",
    "    mean_circuits = combined_df.groupby('l1_lambda')['n_circuits'].mean()\n",
    "    std_circuits = combined_df.groupby('l1_lambda')['n_circuits'].std()\n",
    "    axes[1, 0].errorbar(mean_circuits.index, mean_circuits.values, \n",
    "                       yerr=std_circuits.values, marker='o', linewidth=2, \n",
    "                       markersize=8, capsize=5)\n",
    "    axes[1, 0].set_title('L1: Mean Circuit Count ± SD', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('L1 Lambda')\n",
    "    axes[1, 0].set_ylabel('Mean Circuit Count')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# L2 - Box plot\n",
    "if combined_df['l2_lambda'].nunique() > 1:\n",
    "    sns.boxplot(data=combined_df, x='l2_lambda', y='n_circuits', ax=axes[0, 1], color='orange')\n",
    "    axes[0, 1].set_title('L2: Circuit Count Distribution', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('L2 Lambda')\n",
    "    axes[0, 1].set_ylabel('Number of Circuits')\n",
    "\n",
    "# L2 - Mean trend\n",
    "if combined_df['l2_lambda'].nunique() > 1:\n",
    "    mean_circuits = combined_df.groupby('l2_lambda')['n_circuits'].mean()\n",
    "    std_circuits = combined_df.groupby('l2_lambda')['n_circuits'].std()\n",
    "    axes[1, 1].errorbar(mean_circuits.index, mean_circuits.values,\n",
    "                       yerr=std_circuits.values, marker='o', linewidth=2,\n",
    "                       markersize=8, capsize=5, color='orange')\n",
    "    axes[1, 1].set_title('L2: Mean Circuit Count ± SD', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('L2 Lambda')\n",
    "    axes[1, 1].set_ylabel('Mean Circuit Count')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Dropout - Box plot\n",
    "if combined_df['dropout_rate'].nunique() > 1:\n",
    "    sns.boxplot(data=combined_df, x='dropout_rate', y='n_circuits', ax=axes[0, 2], color='green')\n",
    "    axes[0, 2].set_title('Dropout: Circuit Count Distribution', fontweight='bold')\n",
    "    axes[0, 2].set_xlabel('Dropout Rate')\n",
    "    axes[0, 2].set_ylabel('Number of Circuits')\n",
    "\n",
    "# Dropout - Mean trend\n",
    "if combined_df['dropout_rate'].nunique() > 1:\n",
    "    mean_circuits = combined_df.groupby('dropout_rate')['n_circuits'].mean()\n",
    "    std_circuits = combined_df.groupby('dropout_rate')['n_circuits'].std()\n",
    "    axes[1, 2].errorbar(mean_circuits.index, mean_circuits.values,\n",
    "                       yerr=std_circuits.values, marker='o', linewidth=2,\n",
    "                       markersize=8, capsize=5, color='green')\n",
    "    axes[1, 2].set_title('Dropout: Mean Circuit Count ± SD', fontweight='bold')\n",
    "    axes[1, 2].set_xlabel('Dropout Rate')\n",
    "    axes[1, 2].set_ylabel('Mean Circuit Count')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('all_regularization_effects.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Comprehensive plot saved as 'all_regularization_effects.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "statistical-tests"
   },
   "outputs": [],
   "source": [
    "# Statistical Analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" \"*15 + \"HYPOTHESIS TESTING RESULTS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Get baseline\n",
    "baseline = combined_df[\n",
    "    (combined_df['l1_lambda'] == 0) & \n",
    "    (combined_df['l2_lambda'] == 0) & \n",
    "    (combined_df['dropout_rate'] == 0)\n",
    "]['n_circuits']\n",
    "\n",
    "baseline_mean = baseline.mean()\n",
    "baseline_std = baseline.std()\n",
    "\n",
    "print(f\"BASELINE (No Regularization):\")\n",
    "print(f\"  Mean: {baseline_mean:.2f} ± {baseline_std:.2f}\")\n",
    "print(f\"  N = {len(baseline)}\")\n",
    "print()\n",
    "\n",
    "def test_hypothesis(df, col_name, col_value, baseline, hypothesis_direction):\n",
    "    \"\"\"Test if regularization effect matches hypothesis\"\"\"\n",
    "    treatment = df[df[col_name] == col_value]['n_circuits']\n",
    "    \n",
    "    if len(treatment) == 0:\n",
    "        return None\n",
    "    \n",
    "    # t-test\n",
    "    t_stat, p_val = stats.ttest_ind(baseline, treatment)\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt((baseline.std()**2 + treatment.std()**2) / 2)\n",
    "    cohens_d = (treatment.mean() - baseline.mean()) / pooled_std\n",
    "    \n",
    "    # Mean change\n",
    "    mean_change = treatment.mean() - baseline.mean()\n",
    "    pct_change = 100 * mean_change / baseline.mean()\n",
    "    \n",
    "    # Check if direction matches hypothesis\n",
    "    if hypothesis_direction == 'decrease':\n",
    "        direction_correct = mean_change < 0\n",
    "    else:  # 'increase'\n",
    "        direction_correct = mean_change > 0\n",
    "    \n",
    "    significant = p_val < 0.05\n",
    "    supported = direction_correct and significant\n",
    "    \n",
    "    return {\n",
    "        'mean_change': mean_change,\n",
    "        'pct_change': pct_change,\n",
    "        'p_value': p_val,\n",
    "        'cohens_d': cohens_d,\n",
    "        'n': len(treatment),\n",
    "        'direction_correct': direction_correct,\n",
    "        'significant': significant,\n",
    "        'supported': supported\n",
    "    }\n",
    "\n",
    "# Test L1 hypothesis\n",
    "print(\"=\"*70)\n",
    "print(\"L1 REGULARIZATION (Hypothesis: Sparsity → Fewer Circuits)\")\n",
    "print(\"=\"*70)\n",
    "if combined_df['l1_lambda'].nunique() > 1:\n",
    "    for l1 in sorted(combined_df[combined_df['l1_lambda'] > 0]['l1_lambda'].unique()):\n",
    "        result = test_hypothesis(combined_df, 'l1_lambda', l1, baseline, 'decrease')\n",
    "        if result:\n",
    "            print(f\"\\nL1 = {l1}:\")\n",
    "            print(f\"  Mean change: {result['mean_change']:+.2f} circuits ({result['pct_change']:+.1f}%)\")\n",
    "            print(f\"  Cohen's d: {result['cohens_d']:.3f}\")\n",
    "            print(f\"  p-value: {result['p_value']:.4f}\")\n",
    "            print(f\"  Direction: {'✓ Correct (decrease)' if result['direction_correct'] else '✗ Wrong (increase)'}\")\n",
    "            print(f\"  Significant: {'✓ Yes' if result['significant'] else '✗ No'}\")\n",
    "            print(f\"  Hypothesis: {'✓✓ SUPPORTED' if result['supported'] else '✗✗ NOT SUPPORTED'}\")\n",
    "print()\n",
    "\n",
    "# Test L2 hypothesis\n",
    "print(\"=\"*70)\n",
    "print(\"L2 REGULARIZATION (Hypothesis: Diffusion → More Circuits)\")\n",
    "print(\"=\"*70)\n",
    "if combined_df['l2_lambda'].nunique() > 1:\n",
    "    for l2 in sorted(combined_df[combined_df['l2_lambda'] > 0]['l2_lambda'].unique()):\n",
    "        result = test_hypothesis(combined_df, 'l2_lambda', l2, baseline, 'increase')\n",
    "        if result:\n",
    "            print(f\"\\nL2 = {l2}:\")\n",
    "            print(f\"  Mean change: {result['mean_change']:+.2f} circuits ({result['pct_change']:+.1f}%)\")\n",
    "            print(f\"  Cohen's d: {result['cohens_d']:.3f}\")\n",
    "            print(f\"  p-value: {result['p_value']:.4f}\")\n",
    "            print(f\"  Direction: {'✓ Correct (increase)' if result['direction_correct'] else '✗ Wrong (decrease)'}\")\n",
    "            print(f\"  Significant: {'✓ Yes' if result['significant'] else '✗ No'}\")\n",
    "            print(f\"  Hypothesis: {'✓✓ SUPPORTED' if result['supported'] else '✗✗ NOT SUPPORTED'}\")\n",
    "print()\n",
    "\n",
    "# Test Dropout hypothesis\n",
    "print(\"=\"*70)\n",
    "print(\"DROPOUT (Hypothesis: Redundancy → More Circuits)\")\n",
    "print(\"=\"*70)\n",
    "if combined_df['dropout_rate'].nunique() > 1:\n",
    "    for dr in sorted(combined_df[combined_df['dropout_rate'] > 0]['dropout_rate'].unique()):\n",
    "        result = test_hypothesis(combined_df, 'dropout_rate', dr, baseline, 'increase')\n",
    "        if result:\n",
    "            print(f\"\\nDropout = {dr}:\")\n",
    "            print(f\"  Mean change: {result['mean_change']:+.2f} circuits ({result['pct_change']:+.1f}%)\")\n",
    "            print(f\"  Cohen's d: {result['cohens_d']:.3f}\")\n",
    "            print(f\"  p-value: {result['p_value']:.4f}\")\n",
    "            print(f\"  Direction: {'✓ Correct (increase)' if result['direction_correct'] else '✗ Wrong (decrease)'}\")\n",
    "            print(f\"  Significant: {'✓ Yes' if result['significant'] else '✗ No'}\")\n",
    "            print(f\"  Hypothesis: {'✓✓ SUPPORTED' if result['supported'] else '✗✗ NOT SUPPORTED'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export-results"
   },
   "outputs": [],
   "source": [
    "# Export results\n",
    "combined_df.to_csv('all_results.csv', index=False)\n",
    "print(\"✓ Results saved to 'all_results.csv'\")\n",
    "\n",
    "# Create summary report\n",
    "with open('hypothesis_summary.txt', 'w') as f:\n",
    "    f.write(\"REGULARIZATION EFFECTS ON PARALLEL CIRCUITS\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Baseline: {baseline_mean:.2f} ± {baseline_std:.2f} circuits\\n\\n\")\n",
    "    \n",
    "    # Summarize each hypothesis\n",
    "    for reg_type, col, direction in [('L1', 'l1_lambda', 'decrease'),\n",
    "                                      ('L2', 'l2_lambda', 'increase'),\n",
    "                                      ('Dropout', 'dropout_rate', 'increase')]:\n",
    "        f.write(f\"\\n{reg_type} Hypothesis ({direction}):\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        \n",
    "        if combined_df[col].nunique() > 1:\n",
    "            max_val = combined_df[combined_df[col] > 0][col].max()\n",
    "            result = test_hypothesis(combined_df, col, max_val, baseline, direction)\n",
    "            \n",
    "            if result:\n",
    "                f.write(f\"Strongest effect at {col}={max_val}:\\n\")\n",
    "                f.write(f\"  Change: {result['mean_change']:+.2f} ({result['pct_change']:+.1f}%)\\n\")\n",
    "                f.write(f\"  Effect size: {result['cohens_d']:.3f}\\n\")\n",
    "                f.write(f\"  p-value: {result['p_value']:.4f}\\n\")\n",
    "                f.write(f\"  Status: {'SUPPORTED' if result['supported'] else 'NOT SUPPORTED'}\\n\")\n",
    "\n",
    "print(\"✓ Summary saved to 'hypothesis_summary.txt'\")\n",
    "\n",
    "# Zip everything\n",
    "!zip -r regularization_results.zip logs/ *.png *.csv *.txt\n",
    "\n",
    "print(\"\\n✓ All results packaged!\")\n",
    "print(\"\\nDownloading results...\")\n",
    "\n",
    "from google.colab import files\n",
    "files.download('regularization_results.zip')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Demo: Non-identifiability for a larger network (MNIST dataset)\n",
    "\n",
    "In this notebook, we use the library to demonstrate the non-identifiability of MI criteria for circuit detection at larger scales.\n",
    "\n",
    "To do so, we train a much larger MLP on a subset of MNIST, filtered to contain only images labeled with 0 or 1. This simplified problem allows us to focus on extracting circuits within the last layers of this network, which have a reduced size and are therefore tractable for exhaustive enumeration.\n"
   ],
   "id": "f71c270e71f5ce1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Install/import packages and set parameters",
   "id": "7d829c34f55506a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T13:54:48.934072Z",
     "start_time": "2025-02-05T13:54:48.929390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Uncomment this line to install the library if running on Colab\n",
    "# !pip install git+https://github.com/MelouxM/MI-identifiability.git"
   ],
   "id": "73c2505422788efd",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T13:54:54.177060Z",
     "start_time": "2025-02-05T13:54:48.944186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mi_identifiability.neural_model import MLP\n",
    "from mi_identifiability.circuit import find_circuits\n",
    "from mi_identifiability.utils import set_seeds\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "epochs = 10\n",
    "\n",
    "# MLP parameters\n",
    "input_size = 28 * 28  # MNIST images are 28x28\n",
    "hidden_sizes = [128, 128, 3, 3, 3]  # Example architecture provided\n",
    "output_size = 1  # Regression task with two possible outputs (0 or 1)\n",
    "seed = 42\n",
    "\n",
    "set_seeds(seed)"
   ],
   "id": "2460ffbde8ca86d9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create training and validation datasets\n",
    "\n",
    "We download the full MNIST dataset and filter it to only include images labeled as 0 or 1, then partition it into a training and a validation set."
   ],
   "id": "c727ec49c2050bd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T13:54:54.190702Z",
     "start_time": "2025-02-05T13:54:54.179754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_mnist_data_01():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalize between -1 and 1\n",
    "    ])\n",
    "\n",
    "    # Download the full dataset\n",
    "    mnist_train = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    mnist_val = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "    # Filter only the digits 0 and 1\n",
    "    idx_train = (mnist_train.targets == 0) | (mnist_train.targets == 1)\n",
    "    idx_val = (mnist_val.targets == 0) | (mnist_val.targets == 1)\n",
    "\n",
    "    # Subset datasets\n",
    "    train_subset = Subset(mnist_train, torch.where(idx_train)[0])\n",
    "    val_subset = Subset(mnist_val, torch.where(idx_val)[0])\n",
    "\n",
    "    # Convert targets to float and ensure they are 1D\n",
    "    train_targets = torch.tensor([label for _, label in train_subset]).float()\n",
    "    val_targets = torch.tensor([label for _, label in val_subset]).float()\n",
    "\n",
    "    # Create a new dataset class that combines images and targets\n",
    "    class MNISTSubset(torch.utils.data.Dataset):\n",
    "        def __init__(self, subset, targets):\n",
    "            self.subset = subset\n",
    "            self.targets = targets\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            x, _ = self.subset[index]  # Get the image\n",
    "            y = self.targets[index]  # Get the corresponding target\n",
    "            return x, y\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.subset)\n",
    "\n",
    "    # Create new subsets\n",
    "    train_dataset = MNISTSubset(train_subset, train_targets)\n",
    "    val_dataset = MNISTSubset(val_subset, val_targets)\n",
    "\n",
    "    return train_dataset, val_dataset"
   ],
   "id": "68578d7f265e18bb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluation and training loops\n",
    "\n",
    "The evaluation loop is a standard one. Since our model is an MLP rather than a CNN, each image needs to be flattened into a 1D vector (resulting in a 2D tensor for the entire batch)."
   ],
   "id": "8c66f367c392fbb7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T13:54:54.202172Z",
     "start_time": "2025-02-05T13:54:54.193443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, val_loader, criterion, epoch):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val = x_val.view(x_val.size(0), -1)  # Flatten to 2D for MLP\n",
    "\n",
    "            outputs = model(x_val)\n",
    "\n",
    "            loss = criterion(outputs, y_val.view(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Validation Loss after Epoch [{epoch + 1}]: {avg_val_loss:.4f}\")"
   ],
   "id": "95bfb2f98888b028",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We similarly define a function for training the model:",
   "id": "db35735aa5e72b5b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-05T13:54:54.218533Z",
     "start_time": "2025-02-05T13:54:54.207386Z"
    }
   },
   "source": [
    "def train_model(model, train_loader, val_loader, learning_rate, epochs):\n",
    "    model.train()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}', unit='batch')):\n",
    "            x_batch = x_batch.view(x_batch.size(0), -1)  # Flatten to 2D for MLP\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch.view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        evaluate_model(model, val_loader, criterion, epoch)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can now download the data and create and train the model:",
   "id": "e1b7d3c9e2080786"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T13:55:40.723593Z",
     "start_time": "2025-02-05T13:54:54.223429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset, val_dataset = get_mnist_data_01()\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = MLP(hidden_sizes=hidden_sizes, input_size=input_size, output_size=output_size)\n",
    "train_model(model, train_loader, val_loader, learning_rate, epochs)"
   ],
   "id": "b22e34a0c36ddb37",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 198/198 [00:06<00:00, 32.60batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss after Epoch [1]: 0.2126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 198/198 [00:04<00:00, 45.35batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss after Epoch [2]: 0.1026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 198/198 [00:03<00:00, 51.46batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss after Epoch [3]: 0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 198/198 [00:04<00:00, 48.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss after Epoch [4]: 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 198/198 [00:04<00:00, 48.44batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss after Epoch [5]: 0.0034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 198/198 [00:02<00:00, 76.48batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss after Epoch [6]: 0.0026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 198/198 [00:03<00:00, 60.77batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss after Epoch [7]: 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 198/198 [00:02<00:00, 67.73batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss after Epoch [8]: 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 198/198 [00:03<00:00, 58.10batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss after Epoch [9]: 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 198/198 [00:04<00:00, 39.99batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss after Epoch [10]: 0.0014\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Extracting circuits\n",
    "\n",
    "We are interested in interpreting a submodel consisting in the last three layers of the model. To do so, we need to obtain the inputs of this model, or in other words, record the activations of the previous layer. We do so and store the pre-activations and labels for the entire dataset using the following helper function."
   ],
   "id": "284dbfc116b2d73d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T13:55:40.739561Z",
     "start_time": "2025-02-05T13:55:40.728725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collect_activations(submodel, data_loader):\n",
    "    submodel.eval()\n",
    "    all_activations = []\n",
    "    all_labels = []\n",
    "\n",
    "    inputs_all = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.view(inputs.size(0), -1)  # Flatten images to vectors\n",
    "            inputs_all.append(inputs)\n",
    "            layer_activations = submodel(inputs, return_activations=True)[-1]\n",
    "            all_activations.append(layer_activations)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    # Concatenate activations and labels\n",
    "    all_activations = np.concatenate(all_activations, axis=0)\n",
    "    all_inputs = np.concatenate(inputs_all, axis=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0).unsqueeze(-1)\n",
    "\n",
    "    return torch.tensor(all_inputs, dtype=torch.float32), torch.tensor(all_activations, dtype=torch.float32), all_labels"
   ],
   "id": "644584a0fc5445c4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We then split the model into two parts, with the shallower part used to record activations and the deeper part used for circuit extraction.",
   "id": "863aeb1d9462ba0d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T13:55:41.076695Z",
     "start_time": "2025-02-05T13:55:40.742253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "first_layers = model[:-3]\n",
    "last_layers = model[-3:]\n",
    "\n",
    "x_val, x_val_h, y_val_h = collect_activations(first_layers, val_loader)\n",
    "\n",
    "model.eval()\n",
    "last_layers.eval()"
   ],
   "id": "ef912ff74ff2eec0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (layers): ModuleList(\n",
       "    (0-1): 2 x Sequential(\n",
       "      (0): Linear(in_features=3, out_features=3, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=3, out_features=1, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We then perform a consistency check, making sure that the same outputs (predictions) are obtained when running the original model on the validation data or running the deeper model on the computed pre-activations.",
   "id": "5779163fe2a006c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T13:55:41.096853Z",
     "start_time": "2025-02-05T13:55:41.078741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    original_predictions = model(x_val)\n",
    "    smaller_predictions = last_layers(x_val_h)\n",
    "\n",
    "    rounded_orig = torch.round(original_predictions)\n",
    "    rounded_small = torch.round(smaller_predictions)\n",
    "\n",
    "    correct_predictions_orig = rounded_orig.eq(y_val_h).all(dim=1)\n",
    "    accuracy_orig = correct_predictions_orig.sum().item() / y_val_h.size(0)\n",
    "    print(f\"Accuracy orig: {accuracy_orig}\")\n",
    "\n",
    "    correct_predictions_small = rounded_small.eq(y_val_h).all(dim=1)\n",
    "    accuracy_small = correct_predictions_small.sum().item() / y_val_h.size(0)\n",
    "    print(f\"Accuracy small: {accuracy_small}\")\n",
    "\n",
    "predictions_equal = torch.allclose(original_predictions, smaller_predictions, atol=1e-6)\n",
    "\n",
    "if predictions_equal:\n",
    "    print(\"The predictions from the original MLP and the smaller MLP are the same.\")\n",
    "else:\n",
    "    print(\"The predictions from the original MLP and the smaller MLP differ.\")"
   ],
   "id": "203f79063a7e816",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy orig: 0.9985815602836879\n",
      "Accuracy small: 0.9985815602836879\n",
      "The predictions from the original MLP and the smaller MLP are the same.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, we perform the circuit search on the deeper (and smaller) model.",
   "id": "f10ab220a0055d09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T13:56:08.057185Z",
     "start_time": "2025-02-05T13:55:41.099316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_sk, spar, df = find_circuits(last_layers, x_val_h, y_val_h, accuracy_threshold=0.99)\n",
    "print(f\"Number of circuits: {len(top_sk)}\")"
   ],
   "id": "c0096179291f20ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of circuits: 4702\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The results imply that multiple incompatible where-then-what explanations exist for the larger model. Indeed, while we cannot enumerate explanations for the shallower part of the model, we know that it may allow for either no valid explanations or at least one.\n",
    "\n",
    "If the shallower model has no valid explanation, then no explanation can be found for the entire model. If it has one (or more), then the grounding of the output of this explanation can be taken as the input to the deeper model. This means that each explanation of the shallower model would produce at least one valid grounding (and likely many more) for the entire model for each circuit.\n",
    "\n",
    "As a result, the entire model has either no valid explanation, or at least as many as there are circuits in the deeper model."
   ],
   "id": "e8525f8e9293df91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "923535dc514663d5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
